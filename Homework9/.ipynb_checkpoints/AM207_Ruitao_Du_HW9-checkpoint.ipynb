{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 9 - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ _ _ _ _\n",
    "\n",
    "Verena Kaynig-Fittkau & Pavlos Protopapas <br>\n",
    "Handed out: Wednesday, April 8, 2015<br>\n",
    "Due: 11.59 P.M. Wednesday, April 15, 2015<br>\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to the dropbox.\n",
    "\n",
    "+ Your individual submissions should use the following filename: AM207_YOURNAME_HW9.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format) unless you get permission from the TFs. If you use any special libraries you must include them with your code (program should run as is). \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files and images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW9.tar or AM207_YOURNAME_HW9.zip\n",
    "\n",
    "+ Please remember that your solution should be a report! We would like some explanations of your ideas, and ways to approach the solution. Also please comment your code.\n",
    "\n",
    "** Important **\n",
    "\n",
    "As you are all busy working on your final projects, HW9 and HW10 are half problem sets. This means that each HW has only one problem to solve. There will be a dropbox for each HW, but they will be graded together. If you still have sufficient late days, you can use one late day for HW9 and one late day for HW10. \n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import seaborn as sns\n",
    "#sns.set_style('white')\n",
    "#sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Sam let me be\n",
    "\n",
    "[Green Eggs and Ham](http://en.wikipedia.org/wiki/Green_Eggs_and_Ham) is a best-selling and critically acclaimed children's book by Dr. Seuss, first published on August 12, 1960. As of 2001, according to Publishers Weekly, it was the fourth best-selling English-language children's book of all time.\n",
    "\n",
    "The book is actually the result of a bet between Dr Seuss and his publisher Bennett Cerf. Cerf bet Seuss that he could not complete an entire book using only fifty different words. \n",
    "\n",
    "You can find the entire text of the book in the file eggs.txt. The file is conveniently formatted to not include any punctuation signs. \n",
    "\n",
    "### Part 1: \n",
    "\n",
    "Use the text of the children's book to build a simple statistical language model for the writing style of green eggs and ham. \n",
    "Your model should be a _simple Markov chain of order 1_. Use the model to generate some sentences that are longer than the standard sentences in the file. \n",
    "\n",
    "Note: Each line in the text file is assumed to contain exactly one sentence. \n",
    "\n",
    "* Filter the text to get all fifty unique words of the text\n",
    "* Generate a vector for the starting probabilities of the chain (the first word of the sentence). \n",
    "* Generate a matrix with the transition probabilities of all words. \n",
    "* Add an additional row and column to your matrix to stand for the end of the sentence. Choose whatever symbol you like, I went for a '.' . The last word of a sentence should transition to this symbol and it only transitions to itself. \n",
    "* Generate and print 10 different sequences of 20 states from your model. This should correspond to 10 sentences, which might end in multiple '.', or also not end yet within the 20 states. \n",
    "\n",
    "Hint: It is best to convert all words to lower case. \n",
    "\n",
    "### Part 2:\n",
    "\n",
    "Extend your model to a Hidden Markov Model, that can correct spelling mistakes. The true word is the hidden state $X_i$ and the observed misspelled word is the $Y_i$. Identify the correct spelling by using the Viterbi algorithm to find the most probable chain of states $X$ that caused the observations $Y$. \n",
    "\n",
    "You already have the starting and transition probabilities from part 1. For an HMM you also need emission probabilities. We model the emission probabilities according to a form of edit distance: \n",
    "\n",
    "$$p(Y_i | X_i) = \\begin{cases}\n",
    "                       Poisson(k, \\lambda),& \\text{if } length(Y_i) == length(X_i)\\\\\n",
    "                        0,              & \\text{otherwise}\n",
    "\\end{cases}.$$ \n",
    "\n",
    "Here $k = d(X_i,Y_i)$ is the number of characters that are misspelled, for example $d(Sam, Tom) = 2$. You can play around a bit with the value for $\\lambda$, if you don't want to play around a value of 0.01 should work fine.  \n",
    "\n",
    "Use the Viterbi algorithm to correct the following sentences, and print the corrected version. There is a version of the Viterbi algorithm in the lecture notes that you can use for your reference. Please adjust the code to deal with numerical underflow and add a lot of comments that show you understand what is going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the text of the children's book to build a simple statistical language model for the writing style of green eggs and ham. \n",
    "Your model should be a _simple Markov chain of order 1_. Use the model to generate some sentences that are longer than the standard sentences in the file. \n",
    "\n",
    "Note: Each line in the text file is assumed to contain exactly one sentence. \n",
    "\n",
    "* Filter the text to get all fifty unique words of the text\n",
    "* Generate a vector for the starting probabilities of the chain (the first word of the sentence). \n",
    "* Generate a matrix with the transition probabilities of all words. \n",
    "* Add an additional row and column to your matrix to stand for the end of the sentence. Choose whatever symbol you like, I went for a '.' . The last word of a sentence should transition to this symbol and it only transitions to itself. \n",
    "* Generate and print 10 different sequences of 20 states from your model. This should correspond to 10 sentences, which might end in multiple '.', or also not end yet within the 20 states. \n",
    "\n",
    "Hint: It is best to convert all words to lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filter the text to get all fifty unique words of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'eggs.txt'\n",
    "\n",
    "with open(filename) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# delete \\n of every row\n",
    "for idx, row in enumerate(data):\n",
    "    data[idx] = row[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use set to store every unique word\n",
    "words = set()\n",
    "\n",
    "for row in data:\n",
    "    for word in row.split(' '):\n",
    "        if word!='':\n",
    "            words.add(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a vector for the starting probabilities of the chain (the first word of the sentence).\n",
    "Generate a matrix with the transition probabilities of all words.\n",
    "Add an additional row and column to your matrix to stand for the end of the sentence. Choose whatever symbol you like, I went for a '.' . The last word of a sentence should transition to this symbol and it only transitions to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a map from word to index\n",
    "word_to_index = {}\n",
    "index_to_word = []\n",
    "for idx, word in enumerate(words):\n",
    "    index_to_word.append(word)\n",
    "    word_to_index[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize start probability and transition probability\n",
    "start_prob = np.zeros(51)\n",
    "transit_prob = np.zeros((51,51))\n",
    "\n",
    "for row in data:\n",
    "    row_words = row.split(' ')\n",
    "    for idx, word in enumerate(row_words):\n",
    "        word = word.lower()\n",
    "        \n",
    "        # ignore ''\n",
    "        if word=='':\n",
    "            continue\n",
    "            \n",
    "        # if first word, add to start probability\n",
    "        if idx==0:\n",
    "            prev_idx = word_to_index[word]\n",
    "            start_prob[prev_idx]+=1.\n",
    "            \n",
    "            \n",
    "        # if it is not last word\n",
    "        elif idx<len(row_words)-1:\n",
    "            \n",
    "            transit_prob[prev_idx][word_to_index[word]] += 1.\n",
    "            prev_idx = word_to_index[word]\n",
    "        \n",
    "        # if it is last word\n",
    "        else:\n",
    "            transit_prob[prev_idx][word_to_index[word]] += 1.\n",
    "            prev_idx = None\n",
    "            transit_prob[word_to_index[word]][50] += 1.\n",
    "\n",
    "# '.' goes to itself\n",
    "transit_prob[50][50]=1\n",
    "\n",
    "# normalize them to probability\n",
    "transit_prob /= np.sum(transit_prob, axis = 1)[:,None]\n",
    "start_prob /= np.sum(start_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add '.' to words\n",
    "word_to_index['.']=50\n",
    "index_to_word.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and print 10 different sequences of 20 states from your model. This should correspond to 10 sentences, which might end in multiple '.', or also not end yet within the 20 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_sents = 10\n",
    "num_states = 20\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in xrange(num_sents):\n",
    "    sent = []\n",
    "    for j in xrange(num_states):\n",
    "        if j==0:\n",
    "            prev = np.random.choice(50, p=start_prob)\n",
    "            sent.append(index_to_word[prev])\n",
    "        else:\n",
    "            prev = np.random.choice(51, p=transit_prob[prev])\n",
    "            sent.append(index_to_word[prev])\n",
    "            \n",
    "    sentences.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentences are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i will not in a train . . . . . . . . . . . . . .',\n",
       " 'not could not with a fox . . . . . . . . . . . . . .',\n",
       " 'i am . . . . . . . . . . . . . . . . . .',\n",
       " 'and ham . . . . . . . . . . . . . . . . . .',\n",
       " 'i am . . . . . . . . . . . . . . . . . .',\n",
       " 'not like green eggs and i could you could not like them . . . . . . . .',\n",
       " 'sam i will not like them in a boat . . . . . . . . . . .',\n",
       " 'i do not would not with a goat . . . . . . . . . . . .',\n",
       " 'i would not like that sam let me be . . . . . . . . . . .',\n",
       " 'i will eat them sam let me be i would not on a mouse . . . . . .']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Generated sentences are:\"\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:\n",
    "\n",
    "Extend your model to a Hidden Markov Model, that can correct spelling mistakes. The true word is the hidden state $X_i$ and the observed misspelled word is the $Y_i$. Identify the correct spelling by using the Viterbi algorithm to find the most probable chain of states $X$ that caused the observations $Y$. \n",
    "\n",
    "You already have the starting and transition probabilities from part 1. For an HMM you also need emission probabilities. We model the emission probabilities according to a form of edit distance: \n",
    "\n",
    "$$p(Y_i | X_i) = \\begin{cases}\n",
    "                       Poisson(k, \\lambda),& \\text{if } length(Y_i) == length(X_i)\\\\\n",
    "                        0,              & \\text{otherwise}\n",
    "\\end{cases}.$$ \n",
    "\n",
    "Here $k = d(X_i,Y_i)$ is the number of characters that are misspelled, for example $d(Sam, Tom) = 2$. You can play around a bit with the value for $\\lambda$, if you don't want to play around a value of 0.01 should work fine.  \n",
    "\n",
    "Use the Viterbi algorithm to correct the following sentences, and print the corrected version. There is a version of the Viterbi algorithm in the lecture notes that you can use for your reference. Please adjust the code to deal with numerical underflow and add a lot of comments that show you understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# emission probability\n",
    "def log_emit_p(X, Y):\n",
    "    if len(X)!=len(Y):\n",
    "        return np.log(1e-30)\n",
    "    else:\n",
    "        k = len(set(X)-set(Y))\n",
    "        return sp.stats.poisson.logpmf(k,lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dptable(V):\n",
    "    s = \"    \" + \" \".join((\"%7d\" % i) for i in range(len(V))) + \"\\n\"\n",
    "    for y in V[0]:\n",
    "        s += \"%.5s: \" % y\n",
    "        s += \" \".join(\"%.7s\" % (\"%f\" % v[y]) for v in V)\n",
    "        s += \"\\n\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underflow will appear because we multiply them by too many small value. In order to deal with underflow, I can use log-scale probability to implement the viterbi algorithm. However, we also need to deal with 0 probability in original scale. To deal with 0 probability, I use a small value 1e-30 to approximate 0. Therefore, I recalculate the start probability and transition probability in log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_start_prob = start_prob.copy()\n",
    "log_tran_prob = transit_prob.copy()\n",
    "\n",
    "log_start_prob[log_start_prob==0]=1e-30\n",
    "log_tran_prob[log_tran_prob==0]=1e-30\n",
    "\n",
    "log_start_prob = np.log(log_start_prob)\n",
    "log_tran_prob = np.log(log_tran_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(obs, states, log_start_p, log_trans_p, log_emit_p, print_V=False):\n",
    "    \"\"\"\n",
    "    Input observation, states, start probability, transition probability and emission probability\n",
    "    Output a tuple of probability and true path\n",
    "    \"\"\"\n",
    "    V = [{}]\n",
    "    path = {}\n",
    " \n",
    "    # Initialize base cases (t == 0)\n",
    "    for y, word in enumerate(states):\n",
    "        V[0][word] = log_start_p[y] + log_emit_p(obs[0],word)\n",
    "        path[word] = [word]\n",
    " \n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    " \n",
    "        # iterate every state\n",
    "        for y, word in enumerate(states):\n",
    "            # greedily choose the partial path with maximum probability \n",
    "            (prob, state) = max((V[t-1][word0] + log_trans_p[y0][y] + log_emit_p(obs[t], word), word0) \n",
    "                                for y0, word0 in enumerate(states))\n",
    "            # update to V\n",
    "            V[t][word] = prob\n",
    "            \n",
    "            # add a new word\n",
    "            newpath[word] = path[state] + [word]\n",
    " \n",
    "        # add new path to the old paths\n",
    "        path = newpath\n",
    "    \n",
    "    if print_V:\n",
    "        print_dptable(V)\n",
    "    \n",
    "    # finally, choose the path with maximum probability\n",
    "    (prob, state) = max((V[t][word], word) for y, word in enumerate(states))\n",
    "    return path[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation1 = ('nat','im','a','cor','.')\n",
    "observation2 = ('yoo','lat','ma','be','.')\n",
    "observation3 = ('i', 'do', 'nat', 'eet', 'tnem', 'san', 'i', 'do', 'nat', 'like', 'grean', 'egxs', 'ant', 'hom', '.')\n",
    "observation4 = ('xxx','will','see','.')\n",
    "observation5 = ('do','you','like','xxxxx','xxxx','and','xxx','.')\n",
    "observation6 = ('x', 'xx', 'xxx', 'xxx', 'xxxx', 'xxx', 'x', 'xx', 'xxx', 'xxxx', 'xxxxx', 'xxxx', 'xxx', 'xxx', '.')\n",
    "\n",
    "obs = [observation1, observation2, observation3, observation4, observation5, observation6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       1       2       3       4\n",
      "and: -7.0575 -144.75 -150.89 -97.768 -157.99\n",
      "sam: -13.742 -91.039 -147.96 -97.768 -157.99\n",
      "do: -74.004 -31.181 -147.36 -151.22 -157.99\n",
      "house: -138.15 -143.71 -150.89 -84.165 -157.99\n",
      "am: -138.15 -76.322 -148.22 -151.22 -157.99\n",
      "see: -84.694 -90.345 -150.50 -97.768 -157.99\n",
      "are: -78.991 -143.08 -150.89 -92.064 -157.99\n",
      "a: -72.618 -142.71 -13.073 -151.22 -93.529\n",
      "in: -73.311 -12.741 -148.50 -151.22 -157.99\n",
      "mouse: -138.15 -143.71 -150.89 -84.165 -157.99\n",
      "boat: -138.15 -144.69 -150.89 -85.146 -157.99\n",
      "if: -74.004 -80.290 -150.89 -151.22 -157.99\n",
      "will: -138.15 -80.094 -148.03 -151.22 -157.99\n",
      "ham: -78.991 -77.010 -150.89 -97.768 -157.99\n",
      "would: -71.807 -79.401 -148.29 -151.22 -157.99\n",
      "there: -138.15 -79.313 -150.89 -151.22 -157.99\n",
      "fox: -84.694 -143.84 -150.89 -25.135 -157.99\n",
      ".: -138.15 -82.819 -39.380 -86.245 -19.846\n",
      "green: -138.15 -81.104 -147.20 -151.22 -157.99\n",
      "you: -18.465 -78.619 -103.84 -92.064 -157.99\n",
      "goat: -138.15 -144.40 -150.89 -84.858 -157.99\n",
      "be: -138.15 -85.589 -149.36 -151.22 -157.99\n",
      "them: -138.15 -77.969 -145.87 -151.22 -157.99\n",
      "good: -138.15 -143.30 -150.89 -151.22 -157.99\n",
      "that: -73.311 -144.75 -149.40 -151.22 -157.99\n",
      "may: -78.991 -89.652 -149.80 -97.768 -157.99\n",
      "eggs: -138.15 -144.75 -150.18 -151.22 -157.99\n",
      "here: -73.311 -144.75 -148.94 -151.22 -157.99\n",
      "dark: -138.15 -144.75 -150.89 -151.22 -157.99\n",
      "me: -138.15 -80.290 -150.89 -151.22 -157.99\n",
      "train: -138.15 -143.59 -150.89 -84.047 -157.99\n",
      "let: -78.991 -85.652 -150.50 -97.768 -157.99\n",
      "rain: -138.15 -144.75 -150.89 -151.22 -157.99\n",
      "they: -74.004 -144.75 -150.89 -151.22 -157.99\n",
      "not: -6.5979 -141.57 -100.34 -92.064 -157.99\n",
      "with: -138.15 -78.148 -148.74 -151.22 -157.99\n",
      "eat: -8.8492 -77.696 -150.27 -97.768 -157.99\n",
      "thank: -73.311 -144.75 -150.89 -151.22 -157.99\n",
      "box: -84.694 -143.84 -150.89 -25.135 -157.99\n",
      "on: -138.15 -19.831 -150.50 -151.22 -157.99\n",
      "like: -138.15 -76.539 -149.80 -151.22 -157.99\n",
      "i: -69.927 -77.367 -86.434 -151.22 -93.529\n",
      "car: -78.991 -143.84 -150.89 -19.836 -157.99\n",
      "could: -72.906 -78.302 -150.09 -151.22 -157.99\n",
      "tree: -138.15 -143.99 -150.89 -84.453 -157.99\n",
      "say: -13.742 -91.039 -150.86 -97.768 -157.99\n",
      "try: -13.742 -144.75 -150.89 -92.064 -157.99\n",
      "anywh: -138.15 -144.75 -149.06 -151.22 -157.99\n",
      "so: -73.311 -85.589 -103.84 -151.22 -157.99\n",
      "the: -78.991 -143.68 -83.110 -97.768 -157.99\n",
      "or: -138.15 -83.543 -150.89 -151.22 -157.99\n",
      "\n",
      "Original: ('nat', 'im', 'a', 'cor', '.')\n",
      "Corrected: ['not', 'in', 'a', 'car', '.']\n"
     ]
    }
   ],
   "source": [
    "corrected = viterbi(observation1, index_to_word, log_start_prob, log_tran_prob, log_emit_p, print_V=True)\n",
    "print \"Original:\", observation1\n",
    "print \"Corrected:\", corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot of V above, we can see that we fix the underflow problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Lambda is 0.005. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "When Lambda is 0.010. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "When Lambda is 0.020. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "When Lambda is 0.030. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "When Lambda is 0.050. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "When Lambda is 0.100. The corrected sentences are:\n",
      "['not', 'in', 'a', 'car', '.']\n",
      "['you', 'let', 'me', 'be', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['you', 'will', 'see', '.']\n",
      "['so', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambs = [0.005, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "for value in lambs:\n",
    "    lamb = value\n",
    "    print \"When Lambda is %.3f. The corrected sentences are:\"%lamb\n",
    "    for ob in obs:\n",
    "        print viterbi(ob, index_to_word, log_start_prob, log_tran_prob, log_emit_p)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think $\\lambda $ should be small because when $\\lambda$ is large, the emission probability will dominate the partial path probability. The effect of transition probability is much less and this algorithm will always choose the most similar word. If we have a correct $\\lambda$, we can balance the emission probability and transition probability and find the most possible path. Therefore, according to the output above, I choose my $\\lambda$ as 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final corrected sentences are:\n",
      "not in a car .\n",
      "you let me be .\n",
      "i do not eat them sam i do not like green eggs and ham .\n",
      "you will see .\n",
      "do you like green eggs and ham .\n",
      "i do not eat them sam i do not like green eggs and ham .\n"
     ]
    }
   ],
   "source": [
    "lamb = 0.01\n",
    "print \"Our final corrected sentences are:\"\n",
    "for ob in obs:\n",
    "    print ' '.join(viterbi(ob, index_to_word, log_start_prob, log_tran_prob, log_emit_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "If you would like to extend your knowledge of HMM models with this example, feel free to build models on the level of single characters, and build models of higher order. You could also try different texts, for example Shakespeare's Sonnets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
